{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt # plotting graphs\n",
    "import sklearn\n",
    "%matplotlib inline\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input/ai-academy-intermediate-class-competition-1\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d397f6a850bae7de4f0115791cc3abd58182a9f3"
   },
   "source": [
    "## Load and Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "TRAIN_PATH = os.path.join(\"../input/ai-academy-intermediate-class-competition-1\", \"BBC News Train.csv\")\n",
    "\n",
    "#Load the data using pandas : Create a DataFrame named df, that contains the training data \n",
    "df = pd.read_csv(TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6dcb2f2ffc251270b65d16ba26883e0b6d9bf6c5"
   },
   "outputs": [],
   "source": [
    "# List first 5 entries in dataframe to make sure it was loaded properly\n",
    "# and review the various colums in the dataframe\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bcc06034f96025fe60e8d0d936c419026a8c364d"
   },
   "outputs": [],
   "source": [
    "# Associate Category names with numerical index and save it in new column category_id\n",
    "df['category_id'] = df['Category'].factorize()[0]\n",
    "\n",
    "#View first 10 entries of category_id, as a sanity check\n",
    "df['category_id'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fa0e3a6e8245664473ad44ba14b436d8ca6e5c10"
   },
   "outputs": [],
   "source": [
    "# Create a new pandas dataframe \"category_id_df\", which only has unique Categories, also sorting this list in order of category_id values\n",
    "category_id_df = df[['Category', 'category_id']].drop_duplicates().sort_values('category_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1697e661d0864bb64f42b75e52349c3e8ce48eb9"
   },
   "outputs": [],
   "source": [
    "category_id_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9bd96f79d012a8cec51e00c2139a11901c5c66fc"
   },
   "outputs": [],
   "source": [
    "# Create a dictionary ( python datastructure - like a lookup table) that \n",
    "# can easily convert category names into category_ids and vice-versa\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[['category_id', 'Category']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2e49f9ebdc3b73b043d578d82b4c3d54b7305610"
   },
   "outputs": [],
   "source": [
    "id_to_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "71f4674f1b6380cd1fa4eef88860d753756cf974"
   },
   "outputs": [],
   "source": [
    "# Pick 5 random samples from the dataframe\n",
    "df.sample(5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c79961c3b970b245e03be988e5d9fcda5743997f"
   },
   "outputs": [],
   "source": [
    "# Group the dataframe by categories and count items ( number of news articles) in each category\n",
    "df.groupby('Category').category_id.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "545f01662842adeb019247bbbacc2bdc55a956a6"
   },
   "outputs": [],
   "source": [
    "#Plot the distribution of news articles by category\n",
    "df.groupby('Category').category_id.count().plot.bar(ylim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d1843a95af1d56013396381ecd5eb8aaeeec0958"
   },
   "source": [
    "## Convert words in the news articles into numerical features using tfdif \n",
    "\n",
    "sklearn.feature_extraction.text.TfidfVectorizer will be used to calculate a tf-idf vector for each of our documents. \n",
    "Note that we are passing a number of parameters to this class:\n",
    "\n",
    "*  **sublinear_df** is set to True to use a logarithmic form for frequency, to give diminishing returns as the frequency of a word increases. This is usually preferable for a number of reasons, one of which being Zipf's Law.\n",
    "*  **min_df** is the minimum numbers of documents a word must be present in to be kept, and we are setting it to 5. This is to avoid rare words, which drastically increase the size of our features and might cause overfitting.\n",
    "*  **norm** is set to l2, to ensure all our feature vectors have a euclidian norm of 1. This is helpful for visualizing these vectors, and can also improve (or deteriorate) the performance of some models.\n",
    "* **encoding** is set to latin-1 which is used by our input text.\n",
    "*  **ngram_range** is set to (1, 2) to indicate that we want to consider both unigrams and bigrams, or in other terms: we want to consider single words (\"prices\", \"player\") and pairs of words (\"stock prices\", \"football player\").\n",
    "*  **stop_words** is set to \"english\" to remove all common pronouns (\"a\", \"the\", ...) and further reduce the number of noisy features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4129985f65ec767505628b52777ecb39167e0b51",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "\n",
    "features = tfidf.fit_transform(df.Text).toarray() # Remaps the words in the 1490 articles in the text column of \n",
    "                                                  # data frame into features (superset of words) with an importance assigned \n",
    "                                                  # based on each words frequency in the document and across documents\n",
    "\n",
    "labels = df.category_id                           # represents the category of each of the 1490 articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "91370c5157ee788818bfc9bee7ceb7a0540b33d8"
   },
   "outputs": [],
   "source": [
    "#Get a feel of the features identified by tfidf\n",
    "features.shape # How many features are there ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "620914f91d0a81c7742dcef10639d109dbb35720"
   },
   "outputs": [],
   "source": [
    "# Remember the dictionary created to map category names to a number ? \n",
    "category_to_id.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9630434411a0df01fb6c5e16ccb9a0b1ffa91178"
   },
   "outputs": [],
   "source": [
    "# The sorted function Converts dictionary items into a (sorted) list. \n",
    "# In subsequent steps - We will use this list to iterate over the categories\n",
    "sorted(category_to_id.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f1b081d8a6dd69e8d9e2d43151e4c75882eb4c9b"
   },
   "outputs": [],
   "source": [
    "# Use chi-square analysis to find corelation between features (importantce of words) and labels(news category) \n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "N = 3  # We are going to look for top 3 categories\n",
    "\n",
    "#For each category, find words that are highly corelated to it\n",
    "for Category, category_id in sorted(category_to_id.items()):\n",
    "  features_chi2 = chi2(features, labels == category_id)                   # Do chi2 analyses of all items in this category\n",
    "  indices = np.argsort(features_chi2[0])                                  # Sorts the indices of features_chi2[0] - the chi-squared stats of each feature\n",
    "  feature_names = np.array(tfidf.get_feature_names())[indices]            # Converts indices to feature names ( in increasing order of chi-squared stat values)\n",
    "  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]         # List of single word features ( in increasing order of chi-squared stat values)\n",
    "  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]          # List for two-word features ( in increasing order of chi-squared stat values)\n",
    "  print(\"# '{}':\".format(Category))\n",
    "  print(\"  . Most correlated unigrams:\\n       . {}\".format('\\n       . '.join(unigrams[-N:]))) # Print 3 unigrams with highest Chi squared stat\n",
    "  print(\"  . Most correlated bigrams:\\n       . {}\".format('\\n       . '.join(bigrams[-N:]))) # Print 3 bigrams with highest Chi squared stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2ac23fe5ed75dec2c7afc6b5edafd73db5a71b21"
   },
   "outputs": [],
   "source": [
    "features_chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8471cdc425d8751733ec9a82c9e3e319144a7c7b"
   },
   "source": [
    "## Use t-SNE : A  Dimensionality reduction  technique to visualize ( in 2 diemnsions), a high dimensional space\n",
    "### t-Distributed Stochastc neighbor Embedding : Keeps similar instances close and dissimilar instances apart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "22445bcd76867dd9631d70095f6019e820919d66"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Sampling a subset of our dataset because t-SNE is computationally expensive\n",
    "SAMPLE_SIZE = int(len(features) * 0.3)\n",
    "np.random.seed(0)\n",
    "indices = np.random.choice(range(len(features)), size=SAMPLE_SIZE, replace=False)          # Randomly select 30 % of samples\n",
    "projected_features = TSNE(n_components=2, random_state=0).fit_transform(features[indices]) # Array of all projected features of 30% of Randomly chosen samples \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2de19bd2aeeaa5e8c4becc0b8428c9a65ea8ff3d"
   },
   "outputs": [],
   "source": [
    "type(projected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "80af8591166c708f2e97895dde655fa1a2c8bee1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_id = 0 # Select a category_id\n",
    "projected_features[(labels[indices] == my_id).values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5340a019bd213f512e90bc8fa02b314687d1c12d"
   },
   "source": [
    "### Plot the 2-dimensional ditribution identified by  t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0179726ecdb0a2ff2ec0794c3e813bd0a39a72d6"
   },
   "outputs": [],
   "source": [
    "colors = ['pink', 'green', 'midnightblue', 'orange', 'darkgrey']\n",
    "\n",
    "# Find points belonging to each category and plot them\n",
    "for category, category_id in sorted(category_to_id.items()):\n",
    "    points = projected_features[(labels[indices] == category_id).values]\n",
    "    plt.scatter(points[:, 0], points[:, 1], s=30, c=colors[category_id], label=category)\n",
    "plt.title(\"tf-idf feature vector for each article, projected on 2 dimensions.\",\n",
    "          fontdict=dict(fontsize=15))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "36a688ade47f89aaf1a5b8fe37f79da295613211"
   },
   "source": [
    "# Model Training and Evaluation\n",
    "### We will try 3 different classification models on the data : \n",
    "            Logistic Regression\n",
    "            RandomForestClassifier\n",
    "            MultinomialNB ( Naive Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b9be2f2d7fb2182efa9b2a353a0c4dc491d8ddf6"
   },
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e8ddd2f69d02f69c77aa94b767b31905b059c3ac"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "models = [\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7263091ab7f7efb06b45626fa4f9f6e657cfb653"
   },
   "source": [
    "### Create a data frame that will store the results of various models.\n",
    "Each model will be run 5 times with different test sets of 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9b0784c73236ed16db95a5f0998a9049085d7814"
   },
   "outputs": [],
   "source": [
    "CV = 5  # Cross Validate with 5 different folds of 20% data ( 80-20 split with 5 folds )\n",
    "\n",
    "#Create a data frame that will store the results for all 5 trials of the 3 different models\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = [] # Initially all entries are empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5f8a79ee779b2604bc24cf992d3a1978db7a9977"
   },
   "source": [
    "## Run each Algorithm 5 times and store accuracy results in \"entries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9d56f0aa820fd219f907070b2ea507a1503c1a10"
   },
   "outputs": [],
   "source": [
    "#For each Algorithm \n",
    "for model in models:\n",
    "  model_name = model.__class__.__name__\n",
    "  # create 5 models with different 20% test sets, and store their accuracies\n",
    "  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n",
    "  # Append all 5 accuracies into the entries list ( after all 3 models are run, there will be 3x5 = 15 entries)\n",
    "  for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a154980c0a1928dc8bfb3669d9b54f14dedf74c3"
   },
   "source": [
    "### Store results in the results dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "50ad2ff7f09bb968108e7ff0cbf75dd0dcd33d97"
   },
   "outputs": [],
   "source": [
    "# Store the entries into the results dataframe and name its columns    \n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9133b7e01812723ecc3a6d11f1107e1d02e6327e"
   },
   "source": [
    "### Use seaborn to plot the results\n",
    "\n",
    "seaborn is a library that runs on top of matplotlib and makes drawing fancier plots easier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1b999d466f3862a8671bd2e1d586977a863fd3e6"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
    "              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "681154ea25cead4867405b6e2b467bdb7e9d0ebe"
   },
   "outputs": [],
   "source": [
    "# Mean accuracy of each algorithm\n",
    "cv_df.groupby('model_name').accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ebdb24120fd2bc109ee919c95883872bb70053ef"
   },
   "outputs": [],
   "source": [
    "cv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "37e085dc63373264e263537cdcb5c4ab7a900191"
   },
   "source": [
    "# Model fit Logistic regression with 33% of data randomly chosen for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a6fbb28a3198f2c103237ccb7391121d8fd2a39c"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "model = LogisticRegression(random_state=0)\n",
    "\n",
    "#Split Data \n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.33, random_state=0)\n",
    "\n",
    "#Train Algorithm\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make Predictions\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7230446704d078178510a35c1d07ec64bad522fe"
   },
   "source": [
    "### Print confusion matrix in test data using seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "11a26d305012fc71e83521180b367a4966fb53fc"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=category_id_df.Category.values, yticklabels=category_id_df.Category.values)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "257e7bdcd40c639f76e2357c1b3c3a12a4f5ee55"
   },
   "source": [
    "# Optional\n",
    "### Study the failing scenrios \n",
    "### ****Print the cases where article was miscategorised in same way at least 2 or more times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ec4a8fb63aa0c2a8ef9f86a3bae9d432c77268ac"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "for predicted in category_id_df.category_id:\n",
    "  for actual in category_id_df.category_id:\n",
    "    if predicted != actual and conf_mat[actual, predicted] >= 2:\n",
    "      print(\"'{}' predicted as '{}' : {} examples.\".format(id_to_category[actual], id_to_category[predicted], conf_mat[actual, predicted]))\n",
    "      display(df.loc[indices_test[(y_test == actual) & (y_pred == predicted)]]['Text'])\n",
    "      print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f24c348704e0cd6b3d18e488ef2a927e1aa827f2"
   },
   "source": [
    "# Finally - Use all the data to train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "423b104fd06e1ae2c64cb4c9d9e74d8206f01989"
   },
   "outputs": [],
   "source": [
    "model.fit(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "247dc4635e8de61cd41c34d80ece36b0bb5dcefb"
   },
   "source": [
    "### Print top 5 words / two-word combos for each Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eb7a7fcbb9ab481dac03a37ff92d13e298a2e5d5"
   },
   "outputs": [],
   "source": [
    "# model.coef_ contains the importance of each feature for each category\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "56d68e4a184d3e6e4fcc786bd31822ee836436ed"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "N = 5\n",
    "for Category, category_id in sorted(category_to_id.items()):\n",
    "  indices = np.argsort(model.coef_[category_id])   # This time using the model co-eficients / weights\n",
    "  feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "  unigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 1][:N]\n",
    "  bigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 2][:N]\n",
    "  print(\"# '{}':\".format(Category))\n",
    "  print(\"  . Top unigrams:\\n       . {}\".format('\\n       . '.join(unigrams)))\n",
    "  print(\"  . Top bigrams:\\n       . {}\".format('\\n       . '.join(bigrams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "da5874c000c398bab54190ea38255df42940b572",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "texts = [\"Hooli stock price soared after a dip in PiedPiper revenue growth.\",\n",
    "         \"Captain Tsubasa scores a magnificent goal for the Japanese team.\",\n",
    "         \"Merryweather mercenaries are sent on another mission, as government oversight groups call for new sanctions.\",\n",
    "         \"Beyonc√© releases a new album, tops the charts in all of south-east Asia!\",\n",
    "         \"You won't guess what the latest trend in data analysis is!\"]\n",
    "text_features = tfidf.transform(texts)\n",
    "predictions = model.predict(text_features)\n",
    "for text, predicted in zip(texts, predictions):\n",
    "  print('\"{}\"'.format(text))\n",
    "  print(\"  - Predicted as: '{}'\".format(id_to_category[predicted]))\n",
    "  print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4357ec467b276820364c98d1ac50768f248edf70"
   },
   "source": [
    "1. # Submitting your work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "938d2a59f23c59c198abedc8069426bd5d890e94"
   },
   "outputs": [],
   "source": [
    "print(os.listdir(\"../input/bbc-news-test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "949c21256d7825b36bb2e11d077c49e1793579d8"
   },
   "outputs": [],
   "source": [
    "TEST_PATH = os.path.join(\"../input/bbc-news-test\", \"BBC News Test.csv\")\n",
    "\n",
    "#Load the data using pandas : Create a DataFrame\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0e20aefa5a6d5b9afa45dc3185a9d2091a1a2eab"
   },
   "outputs": [],
   "source": [
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "061bc7127b8309e533d01058d57720dbb392435d"
   },
   "outputs": [],
   "source": [
    "test_features = tfidf.transform(test_df.Text.tolist())\n",
    "\n",
    "Y_pred = model.predict(test_features)\n",
    "\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2db1ae4eff8ecb7f84eebfc1c59646837e3b0807"
   },
   "outputs": [],
   "source": [
    "# Since all predictions are in terms of \"Category IDs (numbers)\", need to convert back to Category name\n",
    "Y_pred_name =[]\n",
    "for cat_id in Y_pred :\n",
    "    Y_pred_name.append(id_to_category[cat_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "275e92564444fac8f7dc613a91c3f5e1ba2b72f1"
   },
   "outputs": [],
   "source": [
    "Y_pred_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "391e128a354f9e42fc9d85d1dc8788bf8b7b3a66"
   },
   "outputs": [],
   "source": [
    "#Create Submission Dataframe\n",
    "submission = pd.DataFrame({\n",
    "        \"ArticleId\": test_df[\"ArticleId\"],\n",
    "        \"Category\": Y_pred_name\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d7f302d499e8e7e05dbadf6e22af1cd0c1a50c5a"
   },
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "39b9006850e22eb0dd1df6f6bc7705b0a32d5084"
   },
   "outputs": [],
   "source": [
    "# Convert submission dataframe to csv \n",
    "# you could use any filename. We choose submission here\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f6418dd6696750c17f4dd30de73d048b23847d66"
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "322d81c2247338658942fe1ea82e4f174524b9c2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
